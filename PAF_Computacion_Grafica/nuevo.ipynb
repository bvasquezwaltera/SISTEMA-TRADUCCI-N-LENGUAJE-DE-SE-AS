{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-14T23:25:08.293868Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Mediapipe holistic model and drawing utilities\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    if results.face_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp.solutions.face_mesh.FACEMESH_TESSELATION)\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    if results.face_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.face_landmarks, mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "            mp_drawing.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),\n",
    "            mp_drawing.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1))\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2))\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "            mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in\n",
    "                     results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in\n",
    "                     results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468 * 3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in\n",
    "                   results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21 * 3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in\n",
    "                   results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21 * 3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "# Data preparation\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "actions = np.array(['hola', 'te_quiero', 'gracias'])\n",
    "no_sequences = 30\n",
    "sequence_length = 30\n",
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "# Model creation\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30, 1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(actions), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "class TQDMProgressCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:\n",
    "            self.epochs = self.params['epochs']\n",
    "            self.progress_bar = tqdm(total=self.epochs, desc='Training Progress')\n",
    "        self.progress_bar.update(1)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.progress_bar.set_postfix(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.progress_bar.close()\n",
    "\n",
    "tqdm_callback = TQDMProgressCallback()\n",
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback, tqdm_callback])\n",
    "model.summary()\n",
    "\n",
    "# Model evaluation\n",
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis=1)\n",
    "yhat = np.argmax(yhat, axis=1)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(ytrue, yhat))\n",
    "print(\"Confusion Matrix:\", multilabel_confusion_matrix(ytrue, yhat))\n",
    "\n",
    "# Visualization\n",
    "colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245)]\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 60 + num * 40), (int(prob * 100), 90 + num * 40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85 + num * 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "# Real-time detection\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            predictions.append(np.argmax(res))\n",
    "\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "                if res[np.argmax(res)] > threshold:\n",
    "                    if len(sentence) > 0:\n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "            \n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "\n",
    "        cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5f633e0f8fb694a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
